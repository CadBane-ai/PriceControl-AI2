# Story 2.2: Implement Frontend Chat Logic

## Status
Done

## Story
As a user, I want my typed messages to appear in the chat history and see a loading indicator, so that I get immediate feedback that the system is processing my query.

## Context
This story builds on Story 2.1’s static chat UI scaffold. It introduces client-side chat logic: appending the user's message to the thread instantly, clearing the input, and showing a loading indicator while a response would be generated. The backend streaming API is not implemented yet (that comes in Story 2.3), but we begin integrating the Vercel AI SDK on the frontend to pave the way.

## Acceptance Criteria
1. Vercel AI SDK is installed and available in the frontend (e.g., `ai` package) and imported in the chat page where appropriate.
2. When a user types a message and clicks "Send" or presses Enter, their message is immediately added to the message display area.
3. After sending a message, the input field is cleared.
4. A loading indicator appears to reflect that the system is processing (e.g., composer button shows spinner and input disabled, and/or thread shows loading state).
5. Chat state (messages array, loading boolean) is managed on the client side only.
6. No network call to any backend API occurs in this story.
7. Tests cover message append-on-send, input clearing, and loading indication.

## Tasks / Subtasks
- [ ] Add the Vercel AI SDK dependency to `apps/web` (e.g., `pnpm add ai`) and ensure it builds under Node 20.x.
- [ ] In `apps/web/app/(protected)/dashboard/page.tsx`, wire `MessageComposer` so that `onSendMessage`:
  - [ ] Immediately appends a new user message to `chatState.messages` with an ID and timestamp.
  - [ ] Clears the input field.
  - [ ] Sets `chatState.loading = true` while simulating processing (no network request yet), then sets `loading = false` after a short timeout or next render tick.
- [ ] Ensure `ChatThread` consumes `messages` and `isLoading` props to render a consistent loading state. Keep `streamingMessage` empty for now.
- [ ] Integrate the Vercel AI SDK on the frontend in a non-calling manner (e.g., import `useChat` from `ai/react` and prepare state wiring), but do not trigger any fetch to a backend route. This may be done by not invoking the hook submit handler yet, or by providing a no-op submit path.
- [ ] Maintain route protection and the existing `(protected)` layout structure.
- [ ] Tests (Vitest + RTL in `apps/web`):
  - [ ] Renders thread and composer.
  - [ ] On submit, a user message appears immediately.
  - [ ] Input clears after send.
  - [ ] Loading indicator becomes visible during processing and then hides.

## Dev Notes
- File Location: Continue using `apps/web/app/(protected)/dashboard/page.tsx` (App Router). Do not introduce `src/app`.
- Components: Reuse `ChatThread` and `MessageComposer` from `apps/web/components/chat/`.
- State: Use existing local state in the page. For this story, do not persist to DB and do not make network calls.
- Vercel AI SDK: Install `ai` and import `useChat` from `ai/react` where appropriate to prepare for Story 2.3 wiring. Avoid calling the backend for now; simply manage the local UI state and loading indicator.
- Accessibility & Responsiveness: Keep prior constraints from Story 2.1 (keyboard access, responsive layouts) intact.
- Architecture References: Next.js App Router, shadcn/ui, Tailwind CSS remain in effect. Backend integration and streaming are deferred to Story 2.3 per PRD.

## Scrum Master Notes
- This story is strictly client-side; QA must confirm no network calls are made.
- The Vercel AI SDK integration is preparatory; actual API route and streaming occurs in Story 2.3.

## Testing
- Manual:
  - Navigate to `/dashboard` and type a message; on Send/Enter, verify the message appears instantly, input clears, and a loading indicator shows briefly.
  - Verify keyboard accessibility remains (input focusable, Send reachable) and layout responsiveness at sm/md/lg.
  - Confirm no network requests are initiated by sending a message (e.g., check browser DevTools network tab).
- Automated:
  - Unit/component tests in `apps/web` verify: message is appended on send, input clears, and loading indicator toggles.

## QA Results

TBD – pending implementation and review.

## PO Validation

PO Review Date: 2025-09-16
Decision: PASS

Validation Summary
- Alignment with PRD Epic 2, Story 2.2 confirmed (docs/prd/7-epic-2-core-llm-interaction-governance.md).
- Scope correctly limits to client-side chat state with immediate user echo, input clear, and loading indicator; no backend calls in this story.
- Prepares Vercel AI SDK usage without invoking network; acceptable as preparatory step for Story 2.3.
- Test expectations are explicit (append-on-send, input clear, loading toggle) and feasible with existing Vitest + RTL setup.
- Pathing and architecture constraints match existing app router structure and components.

Notes/Guidance
- Ensure `useChat` (if imported) does not auto-wire to a route; avoid calling submit handlers that trigger fetch. Keep interactions strictly local.
- Maintain accessibility and responsiveness from 2.1.

## PO Checklist Results
- Mode: YOLO; Project Type: Brownfield with UI
- Result: PASS with 1 PARTIAL
- PARTIAL: Add `ai` (Vercel AI SDK) to `apps/web` deps and pin a compatible version; ensure no network calls occur in 2.2.
- Recommendation: Keep `useChat` local-only; tests must assert message append, input clear, and loading toggle.

## Dev Agent Record
- Assigned To: Dev Agent
- Priority: Medium
- Notes: Implement client-only chat UX with immediate echo and loading indicator; integrate Vercel AI SDK without backend calls.
- File List:
  - `apps/web/app/(protected)/dashboard/page.tsx`
  - `apps/web/__tests__/chat-frontend-logic.test.tsx`
  - `apps/web/vitest.config.ts` (alias for `@`)
  - `apps/web/vitest.setup.ts` (mock ScrollArea for jsdom)
  - `apps/web/package.json` (dependency: `ai`)

## Change Log
- 2025-09-16: Initial draft created based on PRD Epic 2, Story 2.2.
- 2025-09-16: Implemented client-side chat logic, added tests, installed `ai`.
