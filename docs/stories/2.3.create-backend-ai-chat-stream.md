# Story 2.3: Create Backend AI Chat Stream

## Status
Ready for Dev

## Story
As a developer, I want to create a backend API route that streams data from the Cerebras LLM service via OpenRouter, so that the frontend can receive and display the LLM's response.

## Context
This story builds the server endpoint that accepts a chat transcript, calls the LLM, and streams tokens back to the client. It connects the Story 2.2 frontend logic to a real endpoint using the Vercel AI SDK, replacing local-only state updates. Model selection (Story 2.4) will consume the optional `mode` and `model` fields passed through this route.

## Acceptance Criteria
1. An API route exists at `apps/web/app/api/ai/route.ts` (exposed as `/api/ai`) using the Next.js App Router.
2. The route accepts a JSON payload containing the chat `messages` array plus optional `conversationId`, `mode`, and explicit `model` overrides.
3. Requests require an authenticated NextAuth session; unauthenticated calls return `401 Unauthorized`.
4. When `OPENROUTER_API_KEY` is present, the route connects to OpenRouter (Cerebras provider) and streams tokens back to the client; otherwise a deterministic mock streaming response is returned for local development.
5. Streaming responses integrate with the Story 2.2 chat UI using Vercel’s `useChat` hook so the frontend renders incremental tokens.

## Tasks / Subtasks
- [ ] Environment & Documentation
  - [ ] Add `.env.example` entries for `OPENROUTER_API_KEY` (and optional `OPENROUTER_API_URL` if needed) and document setup in `apps/web/README.md`.
- [ ] Implement `/api/ai`
  - [ ] Create `apps/web/app/api/ai/route.ts` with request validation (Zod) covering `messages`, `conversationId`, `mode`, and `model`.
  - [ ] Enforce NextAuth session checks; return 401 when absent.
  - [ ] When OpenRouter credentials are present, call the Cerebras model via Vercel AI SDK and stream responses; otherwise emit a mock stream with diagnostics logged.
  - [ ] Forward `mode` and `model` values to the provider request for future stories.
- [ ] Frontend Wiring
  - [ ] Update `apps/web/app/(protected)/dashboard/page.tsx` to call `/api/ai` through `useChat`, ensuring streaming tokens render in the thread.
  - [ ] Preserve existing loading UX and ensure errors display without regressing Story 2.2 behaviors.
- [ ] Testing
  - [ ] API tests cover authentication gating, OpenRouter call path (mocked), mock fallback, and payload validation failures.
  - [ ] Frontend tests ensure submitting a prompt hits `/api/ai`, streams assistant text, and forwards `mode`/`model` when provided.

## Dev Notes
- Pathing: Next.js App Router file `apps/web/app/api/ai/route.ts`.
- Authentication: Use `getServerSession(authOptions)`; return 401 when missing.
- OpenRouter Integration: Use Vercel AI SDK helpers (`OpenAIStream`, `StreamingTextResponse`) configured with `OPENROUTER_API_KEY` and the Cerebras model group. Include `mode` and `model` in the payload so Story 2.4 can switch models.
- Mock Mode: When env variables are absent, return a deterministic stream (e.g., canned tokens) to keep local development unblocked.
- Persistence: Do not persist messages yet; conversation storage remains future scope.
- Error Handling: Surface provider errors with 5xx responses and include safe diagnostics for logs only.

## Scrum Master Notes
- Keep Story 2.4 (model selection) separate; ensure `mode`/`model` are simply forwarded when provided.
- Do not introduce message persistence; focus on streaming response plumbing.
- Coordinate with Story 2.7 to avoid breaking conversation sidebar UX.

## Testing
- Manual
  - Login, navigate to `/dashboard`, send a prompt; verify tokens stream from `/api/ai` when `OPENROUTER_API_KEY` is configured.
  - Remove or unset `OPENROUTER_API_KEY`; verify mock streaming fallback works.
  - Attempt to call `/api/ai` unauthenticated; expect 401.
- Automated
  - API tests mock OpenRouter to confirm streaming and mock fallback behaviors.
  - Frontend test ensures `useChat` submits to `/api/ai`, forwards `mode`/`model`, and renders streamed output.

## QA Results
TBD – will validate once implementation is complete.

## PO Validation

PO Review Date: 2025-09-17
Decision: PASS with follow-ups

Validation Summary
- Alignment to PRD 2.3 confirmed (docs/prd/7-epic-2-core-llm-interaction-governance.md#story-23-create-backend-ai-chat-stream) including optional `mode`/`model` fields and OpenRouter streaming fallback.
- Architecture compatibility verified with docs/architecture/6-components.md#llm-gateway-service and docs/architecture/8-core-workflows.md#ai-chat-query-with-tool-use.
- Follow-ups: ensure `.env.example` references `OPENROUTER_API_KEY` (not legacy Cerebras key) and API spec path is synced (`/api/ai`).

## PO Checklist Results
- Mode: YOLO; Project Type: Brownfield with UI
- Result: PASS with 2 PARTIAL items
  1. Update `.env.example` and README with `OPENROUTER_API_KEY` guidance.
  2. Confirm API documentation references `/api/ai` consistently (architecture & README).

## Dev Agent Record
- Assigned To: Dev Agent
- Priority: High
- File List:
  - `apps/web/app/api/ai/route.ts`
  - `apps/web/app/(protected)/dashboard/page.tsx`
  - `apps/web/__tests__/api-ai-route.test.ts`
  - `apps/web/__tests__/chat-usechat-integration.test.tsx`

## Change Log
- 2025-09-16: Initial draft created from PRD Epic 2, Story 2.3 with repo-aligned paths and testing plan.
- 2025-09-17: Updated acceptance criteria, tasks, and environment references for OpenRouter + optional `mode/model` per PRD/architecture v4.
